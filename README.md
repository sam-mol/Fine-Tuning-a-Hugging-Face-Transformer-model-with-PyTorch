# Fine-Tuning-a-Hugging-Face-Transformer-model-with-PyTorch

Training a large language model from scratch may not be practical or even possible if we do not have access to large datasets and powerful computers. Using pretrained large language models can help if we want to use the State-of-The-Art language models without having to train one from scratch. Hugging Face Transformers library provides access to thousands of pretrained models for a wide range of NLP tasks. In order to use a pretrained large language model from Hugging Face we can modify the model and retrain it on a dataset specific to our task. This technique is known as fine-tuning, an incredibly powerful approach to use with SOTA NLP models. In this project, I will fine-tune DistilBert, a pretrained model from Hugging Face library, for text classification task. 
Fine-tuning a pretrained model from Hugging Face library could be done by using Hugging Face Trainer API, but in this project I use base PyTorch methods for this task. Using native PyTorch allows me to have better control of the training steps. I found that fine-tuning a pretrained Transformer model is more efficient approach compared to using a Transformer model for feature extraction and then training a simpler classification model on the extracted features, because fine-tuning results in the pretrained model to adjust to our task and our dataset. In another project published here at github I used a Transformer model for feature extraction.
