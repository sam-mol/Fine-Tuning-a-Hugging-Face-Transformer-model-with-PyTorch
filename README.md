# Fine-Tuning-a-Hugging-Face-Transformer-model-with-PyTorch

Training a large language model from scratch may not be practical or even possible if we do not have access to large datasets and powerful computers. Hugging Face Transformers library provides access to thousands of the State-Of-The-Art language models for a wide range of NLP tasks. These SOTA models are pretrained on large text datasets and can be easely applied to common NLP tasks. We can download a pretrained large language model from Hugging Face and modify the model and retrain it on a dataset specific to our task. This technique is known as fine-tuning, an incredibly powerful approach to use with the SOTA NLP models. In this project, I will fine-tune DistilBert, a pretrained model from Hugging Face library. I will use it for text classification task. Fine-tuning a pretrained model from Hugging Face library could be done by using Hugging Face Trainer API, but in this project I use base PyTorch methods for this task. Using native PyTorch methods allows me to have better control of the training steps. I found that fine-tuning a pretrained Transformer model is more efficient approach compared to using a Transformer model for feature extraction and then using the  extracted features to train a simpler classification model.  Fine-tuning a pretrained model results in the model that is adjusted to our task and our dataset. In another project published here at github I used a Transformer model for feature extraction.
